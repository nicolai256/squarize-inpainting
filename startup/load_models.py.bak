import sys
import torch
from torchvision import transforms
from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults
from encoders.modules import BERTEmbedder
import clip
import gc

import argparse, os, sys, glob
from omegaconf import OmegaConf
from PIL import Image
from tqdm import tqdm
import numpy as np
import torch
from main import instantiate_from_config
from ldm.models.diffusion.ddim import DDIMSampler

def loadModels( device,
        model_path="inpaint.pt",
        bert_path="bert.pt",
        kl_path="kl-f8.pt",
        clip_model_name='ViT-L/14',
        steps=None,
        clip_guidance=False,
        cpu=False,
        ddpm=False,
        ddim=False,  
        ckpt , 
        verbose=False):
    pl_sd = torch.load(ckpt, map_location="cpu")
    gs = pl_sd["global_step"]
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd, strict=True)
    model.cuda()
    model.eval()
    return model, gs
    """Loads all ML models and associated variables."""
    model_state_dict = torch.load(model_path, map_location='cpu')

    model_params = {
        'attention_resolutions': '32,16,8',
        'class_cond': False,
        'diffusion_steps': 1000,
        'rescale_timesteps': True,
        'timestep_respacing': '27',  # Modify this value to decrease the number of
                                     # timesteps.
        'image_size': 32,
        'learn_sigma': False,
        'noise_schedule': 'linear',
        'num_channels': 320,
        'num_heads': 8,
        'num_res_blocks': 2,
        'resblock_updown': False,
        'use_fp16': False,
        'use_scale_shift_norm': False,
        'clip_embed_dim': 768 if 'clip_proj.weight' in model_state_dict else None,
        'image_condition': True if model_state_dict['input_blocks.0.0.weight'].shape[1] == 8 else False,
        'super_res_condition': True if 'external_block.0.0.weight' in model_state_dict else False,
    }

    if ddpm:
        model_params['timestep_respacing'] = 1000
    if ddim:
        if steps:
            model_params['timestep_respacing'] = 'ddim'+str(steps)
        else:
            model_params['timestep_respacing'] = 'ddim50'
    elif steps:
        model_params['timestep_respacing'] = str(steps)

    model_config = OmegaConf.load("models/ldm/inpainting_big/config.yaml")
    model_config.update(model_params)

    if cpu:
        model_config['use_fp16'] = False

    # Load models
    model, diffusion = create_model_and_diffusion(**model_config)
    model.load_state_dict(model_state_dict, strict=False)
    model.requires_grad_(clip_guidance).eval().to(device)

    if model_config['use_fp16']:
        model.convert_to_fp16()
    else:
        model.convert_to_fp32()

    def set_requires_grad(model, value):
        for param in model.parameters():
            param.requires_grad = value
    sys.stdout.write(f"Loaded and configured primary model from {model_path}\n")
    sys.stdout.flush()

    model_state_dict = None
    gc.collect()

    # vae
    ldm = torch.load(kl_path, map_location="cpu")
    ldm.to(device)
    ldm.eval()
    ldm.requires_grad_(clip_guidance)
    set_requires_grad(ldm, clip_guidance)
    sys.stdout.write(f"Loaded and configured latent diffusion model from {kl_path}\n")
    sys.stdout.flush()

    gc.collect()

    bert = BERTEmbedder(1280, 32)
    sd = torch.load(bert_path, map_location="cpu")
    bert.load_state_dict(sd)
    bert.to(device)
    bert.half().eval()
    set_requires_grad(bert, False)
    sys.stdout.write(f"Loaded and configured BERT model from {bert_path}\n")
    sys.stdout.flush()

    sd = None
    gc.collect()

    # clip
    clip_model, clip_preprocess = clip.load(clip_model_name, device=device, jit=False)
    clip_model.eval().requires_grad_(False)
    sys.stdout.write(f"Loaded and configured CLIP model from {clip_model_name}\n")
    sys.stdout.flush()
    config = OmegaConf.load("models/ldm/inpainting_big/config.yaml")
    model = instantiate_from_config(config.model)
    model.load_state_dict(torch.load("models/ldm/stable-diffusion-v1/model.ckpt")["state_dict"],
                          strict=False)

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model = model.to(device)
    sampler = DDIMSampler(model)

    os.makedirs(opt.outdir, exist_ok=True)
    with torch.no_grad():
        with model.ema_scope():
            for image, mask in tqdm(zip(images, masks)):
                outpath = os.path.join(opt.outdir, os.path.split(image)[1])
                batch = make_batch(image, mask, device=device)

                # encode masked image and concat downsampled mask
                c = model.cond_stage_model.encode(batch["masked_image"])
                cc = torch.nn.functional.interpolate(batch["mask"],
                                                     size=c.shape[-2:])
                c = torch.cat((c, cc), dim=1)

                shape = (c.shape[1]-1,)+c.shape[2:]
                samples_ddim, _ = sampler.sample(S=opt.steps,
                                                 conditioning=c,
                                                 batch_size=c.shape[0],
                                                 shape=shape,
                                                 verbose=False)
                x_samples_ddim = model.decode_first_stage(samples_ddim)

                image = torch.clamp((batch["image"]+1.0)/2.0,
                                    min=0.0, max=1.0)
                mask = torch.clamp((batch["mask"]+1.0)/2.0,
                                   min=0.0, max=1.0)
                predicted_image = torch.clamp((x_samples_ddim+1.0)/2.0,
                                              min=0.0, max=1.0)

                inpainted = (1-mask)*image+mask*predicted_image
                inpainted = inpainted.cpu().numpy().transpose(0,2,3,1)[0]*255
                Image.fromarray(inpainted.astype(np.uint8)).save(outpath)
    gc.collect()

    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
    return model_params, model, diffusion, ldm, bert, clip_model, clip_preprocess, normalize
